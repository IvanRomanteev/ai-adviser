# --- Azure AI / OpenAI compatible endpoint ---
AZURE_AI_ENDPOINT=https://<YOUR-AI-RESOURCE>.services.ai.azure.com
AZURE_AI_API_KEY=<YOUR_SINGLE_KEY>
AZURE_AI_API_VERSION=2024-10-21

# Deployments
CHAT_DEPLOYMENT=<your-chat-deployment-name>
EMBED_DEPLOYMENT=<your-embedding-deployment-name>

# --- Azure AI Search ---
AZURE_SEARCH_ENDPOINT=https://<YOUR-SEARCH-NAME>.search.windows.net
AZURE_SEARCH_API_KEY=<YOUR-SEARCH-ADMIN-OR-QUERY-KEY>
AZURE_SEARCH_INDEX=<your-index-name>

# optional
TOP_K=8
MAX_CONTEXT_CHARS=12000

# ------------------------------------------------------------------------------
# Observability settings
# ------------------------------------------------------------------------------
# Enable or disable Prometheus metrics exposition.  When disabled the `/metrics`
# endpoint will return 404 and latency instrumentation is skipped.  Defaults to
# "true".  Recommended to keep enabled in production.
METRICS_ENABLED=true

# Enable or disable OpenTelemetry tracing.  When enabled the application will
# emit spans for FastAPI requests and internal RAG stages.  Traces are
# exported via OTLP over HTTP to the endpoint defined by OTLP_ENDPOINT.  When
# disabled the tracing hooks become no‑ops.  Defaults to "false".
TRACING_ENABLED=false

# The OTLP collector endpoint for sending traces, e.g.
# http://localhost:4318/v1/traces or https://tempo.mycompany.com:4318/v1/traces
OTLP_ENDPOINT=

# ------------------------------------------------------------------------------
# Query rewriting
# ------------------------------------------------------------------------------
# Enable or disable the query rewriting step for follow‑up questions.  When
# enabled the model rewrites the current question into a standalone search
# query using the conversation summary and recent messages.  When disabled
# retrieval falls back to concatenating the last user message with the
# current question.  Defaults to "true".
REWRITE_ENABLED=true

# Maximum number of recent messages to provide to the query rewrite model.  Set
# to an integer greater than zero.  Defaults to 3.  Larger values incur more
# tokens and latency.
REWRITE_LAST_N=3

# Maximum number of tokens the query rewrite model may generate.  Defaults to
# 50.
REWRITE_MAX_TOKENS=50

# Temperature for the query rewrite model.  Lower values make the rewrite
# deterministic.  Defaults to 0.0.
REWRITE_TEMPERATURE=0.0

# ------------------------------------------------------------------------------
# Conversation summarisation
# ------------------------------------------------------------------------------
# Enable or disable automatic conversation summarisation.  When enabled the
# service creates a summary after every SUMMARY_EVERY_N messages and stores it
# in the summaries table.  Summaries are used to condense long conversation
# history when building the LLM context.  Defaults to "true".
SUMMARY_ENABLED=true

# Create a new summary after this many messages in a thread.  For example, a
# value of 6 summarises after every six messages.  Defaults to 6.
SUMMARY_EVERY_N=6

# How many of the most recent messages to keep alongside the latest summary
# when building the LLM context.  Defaults to 4.
SUMMARY_KEEP_LAST_K=4

# Maximum tokens allowed for the summary generation.  Summaries should be
# concise to save context budget.  Defaults to 120.
SUMMARY_MAX_TOKENS=120

# Temperature for the summarisation model.  Lower values encourage less
# creative paraphrasing.  Defaults to 0.3.
SUMMARY_TEMPERATURE=0.3

# ------------------------------------------------------------------------------
# Citation enforcement
# ------------------------------------------------------------------------------
# Enforce stricter citation structure validation.  When enabled the
# validator checks that every non‑empty line (outside of the Sources
# section) contains at least one citation and that the Sources section is
# present with mappings.  Defaults to "false" to preserve backward
# compatibility with earlier tests.
CITATION_ENFORCE_STRUCTURE=false
